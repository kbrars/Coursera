{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for array computations and loading data\n",
    "import numpy as np\n",
    "\n",
    "# for building linear regression models and preparing data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(precision=2)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1651.  ,  432.65],\n",
       "       [1691.82,  454.94],\n",
       "       [1732.63,  471.53],\n",
       "       [1773.45,  482.51],\n",
       "       [1814.27,  468.36],\n",
       "       [1855.08,  482.15],\n",
       "       [1895.9 ,  540.02],\n",
       "       [1936.71,  534.58],\n",
       "       [1977.53,  558.35],\n",
       "       [2018.35,  566.42],\n",
       "       [2059.16,  581.4 ],\n",
       "       [2099.98,  596.46],\n",
       "       [2140.8 ,  596.71],\n",
       "       [2181.61,  619.45],\n",
       "       [2222.43,  616.58],\n",
       "       [2263.24,  653.16],\n",
       "       [2304.06,  666.52],\n",
       "       [2344.88,  670.59],\n",
       "       [2385.69,  669.02],\n",
       "       [2426.51,  678.91],\n",
       "       [2467.33,  707.44],\n",
       "       [2508.14,  710.76],\n",
       "       [2548.96,  745.19],\n",
       "       [2589.78,  729.85],\n",
       "       [2630.59,  743.8 ],\n",
       "       [2671.41,  738.2 ],\n",
       "       [2712.22,  772.95],\n",
       "       [2753.04,  772.22],\n",
       "       [2793.86,  784.21],\n",
       "       [2834.67,  776.43],\n",
       "       [2875.49,  804.78],\n",
       "       [2916.31,  833.27],\n",
       "       [2957.12,  825.69],\n",
       "       [2997.94,  821.05],\n",
       "       [3038.76,  833.82],\n",
       "       [3079.57,  833.06],\n",
       "       [3120.39,  825.7 ],\n",
       "       [3161.2 ,  843.58],\n",
       "       [3202.02,  869.4 ],\n",
       "       [3242.84,  851.5 ],\n",
       "       [3283.65,  863.18],\n",
       "       [3324.47,  853.01],\n",
       "       [3365.29,  877.16],\n",
       "       [3406.1 ,  863.74],\n",
       "       [3446.92,  874.67],\n",
       "       [3487.73,  877.74],\n",
       "       [3528.55,  874.11],\n",
       "       [3569.37,  882.8 ],\n",
       "       [3610.18,  910.83],\n",
       "       [3651.  ,  897.42]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/data.csv\",delimiter=',')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,0]\n",
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 1-D arrays into 2-D because the commands later will require it\n",
    "x = np.expand_dims(x, axis=1)\n",
    "y = np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_, y_train, y_ = train_test_split(x,y, test_size=0.40, random_state=1)\n",
    "x_valid, x_test, y_valid, y_test =train_test_split(x_, y_, test_size=0.5, random_state=1)\n",
    "\n",
    "del x_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (30, 1)\n",
      "the shape of the training set (target) is: (30, 1)\n",
      "\n",
      "the shape of the cross validation set (input) is: (10, 1)\n",
      "the shape of the cross validation set (target) is: (10, 1)\n",
      "\n",
      "the shape of the test set (input) is: (10, 1)\n",
      "the shape of the test set (target) is: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_valid.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_valid.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data \n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the class\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "linear_model.fit(x_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406.19374192533127"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed the scaled training set and get the predictions\n",
    "yhat = linear_model.predict(x_train_scaled)\n",
    "\n",
    "mse = mean_squared_error(y_train,yhat)/2\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler, verilerinizi z-skoru ile ölçeklendirmek için kullanılan bir sınıftır. Z-skoru ile ölçeklendirme, her bir özelliğin ortalamasını 0 ve standart sapmasını 1 yapmak için verileri dönüştürür. Bu işlem sırasında fit ve transform adımları şu şekilde çalışır:\n",
    "\n",
    "fit(): Bu metot, eğitim setinin ortalama ve standart sapmasını hesaplar. Ancak, bu aşamada hiçbir veri dönüştürülmez; sadece hesaplama yapılır.\n",
    "\n",
    "transform(): Bu metot, daha önce hesaplanan ortalama ve standart sapmayı kullanarak verilerinizi dönüştürür. Burada eğitim setinin z-skorları hesaplanır.\n",
    "\n",
    "fit_transform(): Bu metot, hem fit() hem de transform() işlemlerini tek bir adımda gerçekleştirir. Ancak, bu durumda, eğitim verisi üzerinde her iki işlem de aynı anda uygulanır.\n",
    "\n",
    "Neden fit_transform() Yerine transform() Kullanmalıyız?\n",
    "Eğitim Seti: İlk olarak, modelinizi eğitim seti üzerinde eğitirsiniz ve bu aşamada StandardScaler ile fit_transform() kullanarak eğitim verinizi ölçeklendirirsiniz. Bu işlemde eğitim verisinin ortalaması ve standart sapması hesaplanır.\n",
    "\n",
    "Kross Validasyon Seti: Kross validasyon setine geldiğinizde, bu veri setini direkt olarak eğitim setinden elde ettiğiniz ortalama ve standart sapma değerleri ile ölçeklendirmelisiniz. Bu nedenle, kross validasyon setini ölçeklendirirken transform() kullanmalısınız. Böylece, kross validasyon setiniz, eğitim setinizin ölçeklendirilmiş versiyonuna uyumlu hale gelir.\n",
    "\n",
    "Eğer fit_transform() kullanırsanız, kross validasyon setinin ortalaması ve standart sapmasını hesaplar ve bu durumda eğitim setindeki değerlerle uyumsuz sonuçlar elde edersiniz. Sonuç olarak, modeliniz yanlış tahminlerde bulunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551.7789026952216"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale cross validation\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "yhat = linear_model.predict(x_valid_scaled)\n",
    "mse = mean_squared_error(y_valid,yhat)/2\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polinom Özellikleri Eklemek: Lineer modelin yetersiz kaldığı durumlarda, modelin performansını artırmak için polinom özellikleri eklemeyi deneyebilirsiniz. Bu, modelin daha karmaşık ilişkileri öğrenmesine yardımcı olabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly =  PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.43  1.47]\n",
      " [-0.28 -0.36]\n",
      " [ 1.71  1.84]\n",
      " [ 0.22  0.11]\n",
      " [ 0.15  0.04]]\n"
     ]
    }
   ],
   "source": [
    "scaler_poly = StandardScaler()\n",
    "x_train_poly_scaled = scaler_poly.fit_transform(x_train_poly)\n",
    "print(x_train_poly_scaled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  49.111609334025154\n",
      "Valid MSE:  87.69841211111911\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train_poly_scaled,y_train)\n",
    "\n",
    "yhat = model.predict(x_train_poly_scaled)\n",
    "training_mse = mean_squared_error(y_train,yhat)/2\n",
    "print(\"Training MSE: \",training_mse)\n",
    "\n",
    "# Valid\n",
    "x_valid_poly = poly.transform(x_valid)\n",
    "x_valid_poly_scaled = scaler_poly.transform(x_valid_poly)\n",
    "yhat = model.predict(x_valid_poly_scaled)\n",
    "valid_mse = mean_squared_error(y_valid,yhat)/2\n",
    "print(\"Valid MSE: \", valid_mse)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
